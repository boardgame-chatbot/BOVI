# bang_rule_qa.py

import faiss
import json
import numpy as np
import os
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from memory_setup import get_memory

# ============================
# 1. ë±… ë°ì´í„°/ì„ë² ë”©/LLM ì¤€ë¹„
# ============================
BANG_DESC_PATH = "data/bang_description.json"     # ë±… ì „ì²´ ë£° ì„¤ëª…
BANG_FAISS_PATH = "data/bang.faiss"              # ë±… ë£°/ì¹´ë“œ ì²­í¬ ë²¡í„°DB
BANG_CHUNKS_PATH = "data/bang_chunks.json"       # ë±… ë£°/ì¹´ë“œ ì²­í¬ í…ìŠ¤íŠ¸

# ë£° ì „ì²´ ì„¤ëª… ë¡œë”©
with open(BANG_DESC_PATH, "r", encoding="utf-8") as f:
    bang_desc = json.load(f)["description"]   # {"description": "...ë£° ì „ì²´..."}

# ë²¡í„°/ì²­í¬ ë¡œë“œ
index = faiss.read_index(BANG_FAISS_PATH)
with open(BANG_CHUNKS_PATH, "r", encoding="utf-8") as f:
    chunks = json.load(f)

embed_model = SentenceTransformer("BAAI/bge-m3", device="cuda")

MODEL_NAME = "kakaocorp/kanana-1.5-2.1b-instruct-2505"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype="auto")
llm_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

memory = get_memory()

# ============================
# 2. ë±… ë£° ìš”ì•½ í•¨ìˆ˜ (í•œë²ˆë§Œ)
# ============================
def summarize_bang_rule():
    prompt = (
        "ë„ˆëŠ” ë±…(BANG!) ë³´ë“œê²Œì„ ë£° ì „ë¬¸ AIì•¼. ì•„ë˜ì˜ ì „ì²´ ë£°ì„ êµ¬ì¡°ì ìœ¼ë¡œ, í•µì‹¬ ê°œë…/ëª©í‘œ/ì§„í–‰ ë°©ì‹/ìŠ¹ë¦¬ ì¡°ê±´ ìœ„ì£¼ë¡œ ì•„ì£¼ ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì¤˜. ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ì´ˆì‹¬ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…í•´.\n\n"
        f"ë£° ì „ì²´:\n{bang_desc}\n\në±… ê²Œì„ì˜ ë£°ì„ ìš”ì•½í•´ì¤˜."
    )
    summary = llm_pipeline(prompt)[0]['generated_text']
    return summary.strip()

# ============================
# 3. ë±… ë£° Q&A í•¨ìˆ˜ (ë°˜ë³µ í˜¸ì¶œ)
# ============================
def bang_rule_qa(user_question):
    # ì„ë² ë”© + ìœ ì‚¬ë„ ê²€ìƒ‰
    q_vec = embed_model.encode([user_question], normalize_embeddings=True)
    _, I = index.search(np.array(q_vec), k=3)
    retrieved_chunks = [chunks[i] for i in I[0]]
    context = "\n\n".join(retrieved_chunks)

    # ë©”ëª¨ë¦¬/ëŒ€í™” ì´ë ¥
    chat_history = memory.load_memory_variables({}).get("chat_history", [])
    chat_history_str = ""
    if chat_history:
        for turn in chat_history:
            chat_history_str += f"User: {turn['content']}\n"

    prompt = (
        "ë„ˆëŠ” ë±…(BANG!) ë³´ë“œê²Œì„ ë£° ì „ë¬¸ AIì•¼. ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:\n"
        "- ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ë£° ì„¤ëª…(context) ë²”ìœ„ ë‚´ì—ì„œë§Œ ìƒì„±í•´.\n"
        "- ë£°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡/ì§€ì–´ë‚´ê¸° ì—†ì´ 'í•´ë‹¹ ì •ë³´ëŠ” ë£°ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë‹µí•´.\n"
        "- ëŒ€í™” ì´ë ¥, ë¬¸ë§¥ì„ ì°¸ê³ í•´ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•´.\n"
        f"\nëŒ€í™” ì´ë ¥:\n{chat_history_str}"
        f"\në£° ì„¤ëª…(context):\n{context}\n\nì§ˆë¬¸: {user_question}\në‹µë³€:"
    )
    answer = llm_pipeline(prompt)[0]['generated_text']
    memory.save_context({"input": user_question}, {"output": answer})
    return answer.strip()

# ============================
# 4. CLI í…ŒìŠ¤íŠ¸ìš© ì¸í„°í˜ì´ìŠ¤
# ============================
def run_bang_cli():
    print("\nğŸ§  ë±… ê²Œì„ ë£° ìš”ì•½ ì„¤ëª… (í•œ ë²ˆë§Œ):\n")
    print(summarize_bang_rule())

    while True:
        user_q = input("\nâ“ ë±… ë£°/ì¹´ë“œ/ì—­í•  ë“±ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
        if user_q.lower() == "q":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        answer = bang_rule_qa(user_q)
        print(f"\nğŸ’¬ ë‹µë³€:\n{answer}")

# ============================
# 5. main ì§„ì…ì 
# ============================
if __name__ == "__main__":
    run_bang_cli()
